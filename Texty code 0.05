### This code will be able to read papers, identify the users typing style, then identify the user 
### Subtract shared values in dictonary, add up total difference and absoulte value. Give a score, small score = similar, large score = differnt
### Scores: Average word length, character freq, word freq, avg sentence lngh, matches of sentences that contain the same two words from both texts
### Gives scores to common vs uncommon, word types, ect

### Make sent len - count ". ", "? ", "! " as punct, (total char - punct) / punct = sent len 
### Check english words, %of common words, tense(He/she/They/Them, You/Yall, I/Me/Mine)

### TO DO - Convert all text files to " " , . ; ! ? format

from cgi import test
import json
from logging import StringTemplateStyle
from multiprocessing.sharedctypes import Value
from pickle import NONE
from re import X
from ssl import ALERT_DESCRIPTION_ACCESS_DENIED
from tkinter import W
from typing import Counter, Dict
    
def breakup(lines_list):
    word_list = []
    text_lines = len(lines_list)
    times = 0
    for i in range(text_lines):
        words = lines_list[times].split()
        times += 1
        word_list = word_list + words
    return word_list
### Turns a block of text read line by line into one list
def avglen(inlist):
    x = 0 
    all_len = []
    words_in_list = len(inlist)
    for i in range(words_in_list):
        cur_len = len(inlist[x])
        all_len.append(cur_len)
        x += 1 
    len_sum = sum(all_len)
    avg_len = len_sum / len(inlist)
    return avg_len
### Returns the average length of a list
def char_pre(testlist):
    all_freq = {}
    t = 0
    l = len(testlist)
    for i in range(l):
        for i in testlist[t]:
            if i in all_freq:
                all_freq[i] += 1
            else:
                all_freq[i] = 1
        t += 1 
    values = all_freq.values()
    total = sum(values)
    per = {}
    for i in all_freq:
        per[i] = float(all_freq[i]/total)
    return per
### Gives the % of each character in a list
def combine_zero_dic(word_list, word_list2):
    wrd_dic_1 = {}
    wrd_dic_1 = char_pre(word_list)
    wrd_dic_2 = {}
    wrd_dic_2 = char_pre(word_list2)
    wrd_dic_1.update({}.fromkeys(wrd_dic_1,0))
    wrd_dic_2.update({}.fromkeys(wrd_dic_2,0))
    full_word_dic = wrd_dic_1 | wrd_dic_2
    return(full_word_dic)
### Creates a dictonary contaning keys from two dictonaries, with all values set to 0
def create_baseline(chrs, full):
    chrs = char_pre(chrs)
    a = Counter(chrs)
    b = Counter(full)
    add_dict = a + b
    dict_3 = dict(add_dict)
    baseline_chrpre = combo | dict_3
    return baseline_chrpre
### Returns a dictonary with a % of all characters in a list, it adds keys from another compared list and sets them to 0
def multiply_dic(new, sample_num):
    for k,v in new.items():
        one_over = 1 / sample_num
        new[k] = v*one_over
        new_multiplied = new
    return new_multiplied
### Divides the values of a dictionary by a sample_num
def multiply_dic2(new, sample_num):
    for k,v in new.items():
        new[k] = v*sample_num
        new_multiplied = new
    return new_multiplied
### multiplies the values of a dictionary by a sample_num
def create_json(new_json, filename):
    j = json.dumps(new_json)
    with open(filename, "w") as f:
        f.write(j)
        f.close()
###Creates new Json
def create_baseline_dict(dict1, dict2):
    base = dict1 | dict2
    base.update({}.fromkeys(base,0))
    print(base)
    return(base)
###Created a dictonary containing keys from 2 dictonaries then sets all values to 0
def merge_dictionaries(dict1, dict2):
    merged_dictionary = {}

    for key in dict1:
        if key in dict2:
            new_value = dict1[key] + dict2[key]
        else:
            new_value = dict1[key]

        merged_dictionary[key] = new_value

    for key in dict2:
        if key not in merged_dictionary:
            merged_dictionary[key] = dict2[key]
    return merged_dictionary
###Adds 2 dictonaries values 
def weighted_avg_of_dic(dictweighted, dictnew, weight):
    weighted = multiply_dic2(dictweighted, weight)
    added_dic = merge_dictionaries(weighted, dictnew)
    we_plus_one = weight + 1
    wavg = multiply_dic(added_dic, we_plus_one)
    return wavg
### Gives a new weighted average of two dict
def wgt_avg_nums(weighted, new, weight):
    multi = weighted * weight
    tot = multi + new
    div = weight + 1 
    avg = tot / div
    return avg
### Gives a weighted average of two numbers 
def add_ending(lis):
    end_list = [" ", ":", ",", ";", "!", "?"]
    fin_concat = []
    for i in range(len(end_list)):
        new_concat = []
        end = end_list[i]
        for e in range(len(lis)):
            newlist = []
            newpart = [" " + lis[e] + end]
            newlist = newlist + newpart
            new_concat = new_concat + newlist
            
        fin_concat = fin_concat + new_concat
    print(fin_concat)
### adds possible endings to words
def add_linespace(file):
    with open(file, "r", encoding="utf8") as f:
                space = [line.rstrip('\n') for line in f]
    for i in range(len(space)):
        space[i] = " " + space[i]
    textfile = open(file, "w", encoding="utf8")
    for element in space:
        textfile.write(element + "\n")
    print(space)
    return space
### Adds a space before every line in a text file
def add_linespace_2(file):
    with open(file, "r", encoding="utf8") as f:
                space = [line.rstrip('\n') for line in f]
    for i in range(len(space)):
        space[i] = space[i] + " "
    textfile = open("C:\Python Projects\Text dump", "a", encoding="utf8")
    for element in space:
        textfile.write(element + "\n")
    print(space)
    return space 
### Changes all of a text file
add_linespace_2("C:\Python Projects\english.txt")
### Takes two dictonaries and a weight for the first and returns a weighted average dictonary
if(1==2):
    wrd_len_diff = abs(avglen(breakup(lines)) - avglen(breakup(lines2)))
    word_list = breakup(lines)
    word_list2 = breakup(lines2)
    combo = combine_zero_dic(word_list, word_list2)
    base = create_baseline(word_list, combo)
    base2 = create_baseline(word_list2, combo)
    for key in base:
        if key in base2:
            base[key] = base[key] - base2[key]
        else:
            pass
    for i in base:
        base[i] = abs(base[i])
    sum_score  = base.values()
    score = sum(sum_score)*100

    wwl = 1
    wcp = .05
    weight_score_wcp = score*wcp 
    weight_score_wwl = wrd_len_diff*wwl

    fin_weight_score = weight_score_wcp + weight_score_wwl
    dict_dump = (char_pre(word_list))        
    filename = "dict_dump_test"
    create_json(dict_dump, filename)
    cur_test_dic_dump = json.load(open("C:\Python Projects\dict_dump_test"))
    sample_num = 100
    multiply_dic(cur_test_dic_dump, sample_num)
else:
    pass
if(1==2):
    Dickens_stats = {"word_length" : 1, "s" : 15}
    j2 = json.dumps(Dickens_stats)
    with open("Dickens_stats_json", "w") as f:
        f.write(j2)
        f.close()
else:
    pass
Dstats_Json = json.load(open("C:\Python Projects\Dickens_stats_json"))

sam_num_json = Dstats_Json["s"]
print("Press t to test a sample text for identification")
print("press f to add a F Scott Fitzgerald text")
part_two_input = input("Press d to add a sapmple for Dickens ")

if(part_two_input == "d"):
    if(sam_num_json == 0):
        with open("Text_Reading\Text_Dickens.txt", encoding="utf8") as df:
            Dickens_text = [line.rstrip('\n') for line in df]
        first_d_json = char_pre(Dickens_text)
        create_json(first_d_json, "DCPjson")
        
        dic_wl_avg = avglen(breakup(Dickens_text))
        print("Average word len in text = ")
        print(dic_wl_avg)
        
        dic_new_stat = {"word_length": dic_wl_avg, "s": 1}
        d1 = json.dumps(dic_new_stat)
        with open("Dickens_stats_json", "w") as f:
            f.write(d1)
            f.close()
        
    else:
        with open("Text_Reading\Text_Dickens.txt", encoding="utf8") as df:
            Dickens_text = [line.rstrip('\n') for line in df]
        new_dtext = char_pre(Dickens_text)
        dweighted_Json = json.load(open("DCPjson"))
        updated_dic = weighted_avg_of_dic(dweighted_Json, new_dtext, sam_num_json)
        create_json(updated_dic, "DCPjson")
        
        dic_wl_avg = avglen(breakup(Dickens_text))
        print(dic_wl_avg)
        d_wrdlenavg_json = Dstats_Json["word_length"]
        new_wrdlenavg = wgt_avg_nums(d_wrdlenavg_json, dic_wl_avg, sam_num_json)
        print("New is")
        print(new_wrdlenavg)
        sample_plus = sam_num_json +1
        
        dic_new_stat = {"word_length": new_wrdlenavg, "s": sample_plus}
        d1 = json.dumps(dic_new_stat)
        with open("Dickens_stats_json", "w") as f:
            f.write(d1)
            f.close()
        
else:
    pass
print("done")


file = open("C:\Python Projects\Text_Reading\Text_Dickens.txt", "r")
data = file.read()
occurrences = data.count(".") + data.count("!") + data.count("?")
print('Number of occurrences of the word :', occurrences)
number_of_characters = len(data)
print('Number of characters in text file :', number_of_characters)
sentence_len_avg = (number_of_characters - occurrences) / occurrences
print(sentence_len_avg)
### Gets the chars per sentence, 


text_tense_test = "C:\Python Projects\Text_Reading\Text_Fscot.txt"

def tense_ratio(text_tense_test):
    file = open(text_tense_test, "r", encoding="utf8")
    data = file.read()

    first_tense_words = [" I", " Me", " Mine", " Our", " Ours", " Us", " We", " me", " mine", " our", " ours", " us", " we"]

    ftlen = len(first_tense_words)
    ftcount = 0
    for i in range(ftlen):
        ftcount = ftcount + data.count(first_tense_words[i])
        
    second_tense_words = [" You", " Your", " Yours", " you", " your", " yours"]
    stlen = len(second_tense_words)
    stcount = 0
    for i in range(stlen):
        stcount = stcount + data.count(second_tense_words[i])

    third_tense_words = [" He", " Him", " She", " Her", " His", " Hers", " It", " Its", " They", " Them", " Their", " Theirs", " he", " him", " she", " her", " his", " hers", " it", " its", " they", " them", " their", " theirs"]
    ttlen = len(third_tense_words)
    ttcount = 0
    for i in range(ttlen):
        ttcount = ttcount + data.count(third_tense_words[i])

    tot_ten = ftcount + stcount + ttcount
    print(tot_ten)
    f_ratio = ftcount / tot_ten
    s_ratio = stcount / tot_ten
    t_ratio =  ttcount / tot_ten
    tense_ratio = {"First tense " : f_ratio, "Second tense" : s_ratio , "Third tense" : t_ratio}
    print(tense_ratio)
    return tense_ratio
### Finds the ratio to tenses used in the text

tense_ratio(text_tense_test)

### Finds how common the words used are 
### Def common ratio and word part scores 

with open("Text_Reading\Text_Dickens.txt", encoding="utf8") as lc_dic:
            dic_lower = [line.rstrip('\n') for line in lc_dic]
for i in range(len(dic_lower)):
    dic_lower[i] = dic_lower[i].lower()
textfile = open("C:\Python Projects\Text_Reading\lowercase.txt", "w", encoding="utf8")
for element in dic_lower:
    textfile.write(element + "\n")
textfile.close()

english_list = ["C:\Python Projects\english.txt", "C:\Python Projects\english5000", "C:\Python Projects\english2000.txt", "C:\Python Projects\english500", "C:\Python Projects\english200", "C:\Python Projects\english50", "C:\Python Projects\english20", "C:\Python Projects\list of adjectives.txt", "C:\Python Projects\List of Adverbs.txt", "C:\Python Projects\list of verbs.txt"]
en_ls = 0
while(en_ls <= 9):
    with open(english_list[en_ls], encoding="utf8") as enn:
                english = [line.rstrip('\n') for line in enn]
    enwrd = open("C:\Python Projects\Text_Reading\lowercase.txt", "r")
    data = enwrd.read()
    english_count = 0
    for i in range(len(english)):
        testword = english[i]
        english_count = data.count(testword) + english_count
    print(english_count)
    en_ls = en_ls + 1
textfile.close()
print("test")
def punc_ratio(text_punc_test):

    file = open(text_punc_test, "r", encoding="utf8")
    data = file.read()

    period = "."
    period_count = data.count(period)
    print(period_count)

    exclam = "!"
    exclam_count = data.count(exclam)
    print(exclam_count)

    ques = "?"
    ques_count = ques.count(ques)
    print(ques_count)

    com = ","
    com_count = data.count(com)
    print(com_count)

    semicol = ";"
    semicol_count = data.count(semicol)
    print(semicol_count)

    punc_tot = period_count + exclam_count + ques_count + com_count + semicol_count
    punc_ratio = {}
    punc_ratio = {"Period" : period_count/punc_tot, "Exclamation" : exclam_count/punc_tot, "Question" : ques_count/punc_tot, "Comma" : com_count/punc_tot, "Semi colon" : semicol_count/punc_tot}
    print(punc_ratio)
    return punc_ratio
### Gets the ratio of punctuation in a text

punc_ratio("C:\Python Projects\Text_Reading\Text_Dickens.txt")

def common_ratio(test_common_test): 

    with open(test_common_test, encoding="utf8") as lc_dic:
                dic_lower = [line.rstrip('\n') for line in lc_dic]
    for i in range(len(dic_lower)):
        dic_lower[i] = dic_lower[i].lower()
    textfile = open("C:\Python Projects\Text_Reading\lowercase.txt", "w", encoding="utf8")
    for element in dic_lower:
        textfile.write(element + "\n")
    textfile.close()

    english_list = ["C:\Python Projects\english.txt", "C:\Python Projects\english5000", "C:\Python Projects\english2000.txt", "C:\Python Projects\english500", "C:\Python Projects\english200", "C:\Python Projects\english50", "C:\Python Projects\english20"]
    en_ls = 0
    com_ratio = []
    while(en_ls <= 6):
        with open(english_list[en_ls], encoding="utf8") as enn:
                    english = [line.rstrip('\n') for line in enn]
        enwrd = open("C:\Python Projects\Text_Reading\lowercase.txt", "r", encoding="utf8")
        data = enwrd.read()
        english_count = 0
        for i in range(len(english)):
            testword = english[i]
            english_count = data.count(testword) + english_count
        print(english_count)
        en_ls = en_ls + 1
        com_ratio.append(english_count)
    print(com_ratio)
    textfile.close()
    return com_ratio
### Gets the ratios of common vs uncommon words  
    

common_ratio("C:\Python Projects\Text_Reading\Text_Fscot.txt")

def word_type_ratio(test_type_text):
    
    with open(test_type_text, encoding="utf8") as lc_dic:
                dic_lower = [line.rstrip('\n') for line in lc_dic]
    for i in range(len(dic_lower)):
        dic_lower[i] = dic_lower[i].lower()
    textfile = open("C:\Python Projects\Text_Reading\lowercase.txt", "w", encoding="utf8")
    for element in dic_lower:
        textfile.write(element + "\n")
    textfile.close()

    english_list = ["C:\Python Projects\list of adjectives.txt", "C:\Python Projects\List of Adverbs.txt", "C:\Python Projects\list of verbs.txt"]
    en_ls = 0
    com_ratio = []
    while(en_ls <= 2):
        with open(english_list[en_ls], encoding="utf8") as enn:
                    english = [line.rstrip('\n') for line in enn]
        enwrd = open("C:\Python Projects\Text_Reading\lowercase.txt", "r", encoding="utf8")
        data = enwrd.read()
        english_count = 0
        for i in range(len(english)):
            testword = english[i]
            english_count = data.count(testword) + english_count
        print(english_count)
        en_ls = en_ls + 1
        com_ratio.append(english_count)
    print(com_ratio)
    textfile.close()
    return com_ratio
### Gets ratios for word types 

word_type_ratio("C:\Python Projects\Text_Reading\Text_Fscot.txt")

### Def adjust ratio 

with open("Text_Reading\Text_Dickens.txt", encoding="utf8") as df:
    Dickens_text = [line.rstrip('\n') for line in df]
        
dic_wl_avg = avglen(breakup(Dickens_text))
print("Average word len in text = ")
print(dic_wl_avg)
file.close

file = open("Text_Reading\Text_Dickens.txt", "r")
#read the content of file
data = file.read()
#get the length of the data
number_of_characters = len(data)
print('Number of characters in text file :', number_of_characters)
word_numbers = number_of_characters / dic_wl_avg
print(word_numbers)
