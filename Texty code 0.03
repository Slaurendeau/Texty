### This code will be able to read papers, identify the users typing style, then identify the user 
### Note to self: for i in word_list = for "whatver / whatvers" in word list
### Sentence lngh function
### Make function that will give a % value as a decimal for each character 
### Make a function that gives the % value each word takes up
### Subtract shared values in dictonary, add up total difference and absoulte value. Give a score, small score = similar, large score = differnt
### Scores: Average word length, character freq, word freq, avg sentence lngh, matches of sentences that contain the same two words from both texts
### Gives scores to common vs uncommon, word types, ect
from cgi import test
import json
from multiprocessing.sharedctypes import Value
from pickle import NONE
from re import X
from tkinter import W
from typing import Counter, Dict

print("Press t to test a text")
### Tests an example to determine what author it is from
print("Press d to add a dickens example")
### Adds an example text for Dickens
print("Press f to add a fscot example")
### Adds an example text for Dickens
file_choice = input("What would you like to do?")

if(file_choice == "copy"):
    name_of_file = "C:\Python Projects\Text_Reading\copypasta.txt"
    
elif(file_choice == "will"):
    name_of_file = "C:\Python Projects\Text_Reading\will_the_real_marshviperx_please_stand_up.txt"
    
else:
    name_of_file = "C:\Python Projects\Text_Reading\Text_Dickens.txt"

with open(name_of_file, encoding="utf8") as file:
    lines = [line.rstrip('\n') for line in file]

file_choice2 = input("What file would you like to use?")

if(file_choice2 == "copy"):
    name_of_file2 = "C:\Python Projects\Text_Reading\copypasta.txt"
    
elif(file_choice == "will"):
    name_of_file2 = "C:\Python Projects\Text_Reading\will_the_real_marshviperx_please_stand_up.txt"
    
else:
    name_of_file2 = "C:\Python Projects\Text_Reading\Text_Fscot.txt"

with open(name_of_file2, encoding="utf8") as file2:
    lines2 = [line.rstrip('\n') for line in file2]
    
def breakup(lines_list):
    word_list = []
    text_lines = len(lines_list)
    times = 0
    for i in range(text_lines):
        words = lines_list[times].split()
        times += 1
        word_list = word_list + words
    return word_list
### Turns a block of text read line by line into one list
def avglen(inlist):
    x = 0 
    all_len = []
    words_in_list = len(inlist)
    for i in range(words_in_list):
        cur_len = len(inlist[x])
        all_len.append(cur_len)
        x += 1 
    len_sum = sum(all_len)
    avg_len = len_sum / len(inlist)
    return avg_len
### Returns the average length of a list
def char_pre(testlist):
    all_freq = {}
    t = 0
    l = len(testlist)
    for i in range(l):
        for i in testlist[t]:
            if i in all_freq:
                all_freq[i] += 1
            else:
                all_freq[i] = 1
        t += 1 
    values = all_freq.values()
    total = sum(values)
    per = {}
    for i in all_freq:
        per[i] = float(all_freq[i]/total)
    return per
### Gives the % of each character in a list
def combine_zero_dic(word_list, word_list2):
    wrd_dic_1 = {}
    wrd_dic_1 = char_pre(word_list)
    wrd_dic_2 = {}
    wrd_dic_2 = char_pre(word_list2)
    wrd_dic_1.update({}.fromkeys(wrd_dic_1,0))
    wrd_dic_2.update({}.fromkeys(wrd_dic_2,0))
    full_word_dic = wrd_dic_1 | wrd_dic_2
    return(full_word_dic)
### Creates a dictonary contaning keys from two dictonaries, with all values set to 0
def create_baseline(chrs, full):
    chrs = char_pre(chrs)
    a = Counter(chrs)
    b = Counter(full)
    add_dict = a + b
    dict_3 = dict(add_dict)
    baseline_chrpre = combo | dict_3
    return baseline_chrpre
### Returns a dictonary with a % of all characters in a list, it adds keys from another compared list and sets them to 0
def multiply_dic(new, sample_num):
    for k,v in new.items():
        one_over = 1 / sample_num
        new[k] = v*one_over
        new_multiplied = new
    return new_multiplied
### Multiplies the values of a dictionary by a sample_num
def create_json(new_json, filename):
    j = json.dumps(new_json)
    with open(filename, "w") as f:
        f.write(j)
        f.close()
###Creates new Json
def create_baseline_dict(dict1, dict2):
    base = dict1 | dict2
    base.update({}.fromkeys(base,0))
    print(base)
    return(base)
###Created a dictonary containing keys from 2 dictonaries then sets all values to 0
def merge_dictionaries(dict1, dict2):
    merged_dictionary = {}

    for key in dict1:
        if key in dict2:
            new_value = dict1[key] + dict2[key]
        else:
            new_value = dict1[key]

        merged_dictionary[key] = new_value

    for key in dict2:
        if key not in merged_dictionary:
            merged_dictionary[key] = dict2[key]
    return merged_dictionary
###Adds 2 dictonaries values 
def weighted_avg_of_dic(dictweighted, dictnew, weight):
    wgh = 1 / weight
    weighted = multiply_dic(dictweighted, wgh)
    added_dic = merge_dictionaries(weighted, dictnew)
    x = weight
    div = x
    wavg = multiply_dic(added_dic, div)
    return wavg
### Takes two dictonaries and a weight for the first and returns a weighted average dictonary
wrd_len_diff = abs(avglen(breakup(lines)) - avglen(breakup(lines2)))

word_list = breakup(lines)
word_list2 = breakup(lines2)
combo = combine_zero_dic(word_list, word_list2)
base = create_baseline(word_list, combo)
base2 = create_baseline(word_list2, combo)
for key in base:
    if key in base2:
        base[key] = base[key] - base2[key]
    else:
        pass
for i in base:
    base[i] = abs(base[i])
sum_score  = base.values()
score = sum(sum_score)*100

### Above we get the word ln diff, and the % of char diff
### Under we will use weights, and given 2 texts try to determine the author of the 3rd

###loop above code for an input, find the lowest weighted score

wwl = 1
wcp = .05
weight_score_wcp = score*wcp 
weight_score_wwl = wrd_len_diff*wwl

fin_weight_score = weight_score_wcp + weight_score_wwl
dict_dump = (char_pre(word_list))        
filename = "dict_dump_test"
create_json(dict_dump, filename)
cur_test_dic_dump = json.load(open("C:\Python Projects\dict_dump_test"))

### Read and write dictonary as JSON file

sample_num = 100
### sample number = number of samples from before

multiply_dic(cur_test_dic_dump, sample_num)

cur_test2 = json.load(open("C:\Python Projects\Dickens_stats_json"))
json_dict_dump = json.load(open("C:\Python Projects\dict_dump_test"))

###Next step, be able to add Chr% and wordlen and number of samples to the "Dickens stats"

w = 7
print("dict 1")
print(char_pre(word_list))
print("dict 2")
print(char_pre(word_list2))
print("Weighted avg")
p = weighted_avg_of_dic((char_pre(word_list)), char_pre(word_list2), w)
print(p)

### To do next, read text for dickins, make weighted average dictonary and store it, +1 to samples number
### Files - Dickens stats, Dickens test, Dickens weighted dic, General Test, Fscot stats, Fscot weighted, Fscot test

Dickens_stats = {"word_length" : 1, "s" : 10}

j2 = json.dumps(Dickens_stats)
with open("Dickens_stats_json", "w") as f:
    f.write(j2)
    f.close()

Dstats_Json = json.load(open("C:\Python Projects\Dickens_stats_json"))
sam_num_json = Dstats_Json["s"]

part_two_input = input("Press d to add a sapmple for Dickens")
if(part_two_input == "d"):
    if(sam_num_json == 0):
        with open("Text_Reading\Text_Dickens.txt", encoding="utf8") as df:
            Dickens_text = [line.rstrip('\n') for line in df]
        print("Text = ")
        print(char_pre(Dickens_text))
        first_d_json = char_pre(Dickens_text)
        create_json(first_d_json, "DCPjson")
        
    else:
        with open("Text_Reading\Text_Dickens.txt", encoding="utf8") as df:
            Dickens_text = [line.rstrip('\n') for line in df]
            new_dtext = char_pre(Dickens_text)
            print("Test")
            print(new_dtext)
            dweighted_Json = json.load(open("DCPjson"))
            print(dweighted_Json)
            updated_dic = weighted_avg_of_dic(dweighted_Json, new_dtext, 6)
            print(updated_dic)
else:
    pass

### weighted_avg_of_dic does not work, check merge and multiply 
